{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your video file\n",
    "video_path_8min = '/Users/emilyng/Documents/CP3108B/SliTraNet/videos/test/trimmed_vid_lec_03.mp4'\n",
    "video_path_30min = '/Users/emilyng/Documents/CP3108B/SliTraNet/videos/test/ocw_11165_lecture06_2022sep28-1325_360p_16_9.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torchvision.io.video as video\n",
    "import torch.nn.functional as F\n",
    "\n",
    "## kernel dies if I test on videos that are ~ 30 mins long\n",
    "\n",
    "# Read the video and convert it into PyTorch tensors\n",
    "frames_2, audio_2, info_2 = video.read_video(video_path_2, pts_unit=\"sec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14603"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frames) # 14603 frames for 8 min video, took 2m 7.3s to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([360, 640, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames[4].shape # torch.Size([360, 640, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Open CV to read videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8 mins is also 14603 frames, and can apply torch.as_tensor() on frame to convert to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of frames returned is 14603\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def get_frames_as_tensor(path, library='MoviePy', frame_per_sec= None,):\n",
    "    '''\n",
    "    input: video path name\n",
    "    output: list of video frames in tensor format\n",
    "    '''\n",
    "    frame_as_tensor = []\n",
    "\n",
    "    if library=='MoviePy':\n",
    "        # Create a VideoFileClip object from the input video\n",
    "        video_clip = VideoFileClip(path) # returns video.io VideoFileClip \n",
    "        # default is sth like 30fps, can check using video_clip.fps\n",
    "        if not frame_per_sec:\n",
    "            frame_per_sec = video_clip.fps \n",
    "\n",
    "        for frame in video_clip.iter_frames(fps=frame_per_sec): \n",
    "            frame_as_tensor.append(torch.as_tensor(frame))\n",
    "    \n",
    "    elif library=='cv2': #cv2\n",
    "        cap = cv2.VideoCapture(path)\n",
    "        length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) \n",
    "        for i in range(length):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i) # starts counting from 0\n",
    "            ret, frame = cap.read()\n",
    "            frame_as_tensor.append(torch.as_tensor(frame))\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"set library='MoviePy' or 'cv2' \")\n",
    "    print(f\"number of frames returned is {len(frame_as_tensor)}\")\n",
    "    return frame_as_tensor\n",
    "\n",
    "lst_8min = get_frames_as_tensor(video_path_8min, 'MoviePy')\n",
    "# Release the video capture object when done\n",
    "#cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "cv2.imwrite('/Users/emilyng/Documents/CP3108B/moviepy_trimmed_static_slide_5_361.jpg', np.array(lst_8min[360]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]],\n",
       "\n",
       "        [[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]],\n",
       "\n",
       "        [[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]],\n",
       "\n",
       "        [[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]],\n",
       "\n",
       "        [[ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         ...,\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162],\n",
       "         [ 54,   0, 162]]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_30min[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to write array to jpg and display\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 64355)\n",
    "ret, frame = cap.read()\n",
    "cv2.imwrite('/Users/emilyng/Documents/CP3108B/moviepy_trimmed_frame_49.jpg', frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using MoviePy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Create a VideoFileClip object from the input video\n",
    "video_clip = VideoFileClip(video_path_30min) # returns video.io VideoFileClip # 30 fps\n",
    "print(video_clip.fps)\n",
    "frame_as_tensor = []\n",
    "for frame in video_clip.iter_frames(fps=10):  # instead of video_clip.fps\n",
    "    frame_as_tensor.append(torch.as_tensor(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21452"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "len(frame_as_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([360, 640, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_as_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_as_tensor_3fps = []\n",
    "for frame in video_clip.iter_frames(fps=3):\n",
    "    frame_as_tensor_3fps.append(torch.as_tensor(frame))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1461"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(frame_as_tensor_3fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(video_clip.iter_frames(fps=video_clip.fps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_clip.fps # 30 fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30 * 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize an empty list to store the PyTorch tensors\n",
    "frames_as_tensors = []\n",
    "\n",
    "# Define a transformation to convert frames into tensors\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Iterate through the frames in the video\n",
    "for frame in video_clip.iter_frames(fps=video_clip.fps):\n",
    "    # Convert the frame to a PyTorch tensor and append it to the list\n",
    "    frame_tensor = transform(frame)\n",
    "    frames_as_tensors.append(frame_tensor)\n",
    "\n",
    "# Convert the list of tensors to a PyTorch tensor\n",
    "frames_as_tensors = torch.stack(frames_as_tensors)\n",
    "\n",
    "# 'frames_as_tensors' now contains all the frames as PyTorch tensors\n",
    "# You can process, analyze, or use these tensors for your specific task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch changing width & height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Desired width and height\n",
    "desired_width = 640\n",
    "desired_height = 480\n",
    "# Resize each frame to the desired dimensions\n",
    "resized_frames = [F.interpolate(frame.permute(2, 0, 1).unsqueeze(0), size=(desired_height, desired_width), mode=\"bilinear\", align_corners=False).squeeze(0).permute(1, 2, 0) for frame in frames]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
